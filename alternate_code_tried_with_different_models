# from flask import Flask, render_template,request
# import cv2 
# from src.utils.all_utils import allowed_file
# import os 
# # from tensorflow.keras.applications.vgg16 import preprocess_input
# # from tensorflow.keras.applications.vgg16 import VGG16
# from src.utils.all_utils import read_yaml, create_directory
# import pickle
# from sklearn.metrics.pairwise import cosine_similarity
# import streamlit as st
# from PIL import Image
# import numpy as np
# import face_recognition
# from tensorflow.keras.applications import ResNet50
# from tensorflow.keras.applications.resnet50 import preprocess_input

# config = read_yaml('config/config.yaml')
# params = read_yaml('params.yaml')

# artifacts = config['artifacts']
# artifacts_dir = artifacts['artifacts_dir']
# static_dir=artifacts['static_dir']
# #upload
# upload_image_dir = artifacts['upload_image_dir']#the folder where uploaded image will be stored ,uploads
# uploadn_path = os.path.join(static_dir, upload_image_dir)

# # pickle_format_data_dir
# pickle_format_data_dir = artifacts['pickle_format_data_dir']
# img_pickle_file_name = artifacts['img_pickle_file_name']

# raw_local_dir_path = os.path.join(artifacts_dir, pickle_format_data_dir)
# pickle_file = os.path.join(raw_local_dir_path, img_pickle_file_name)

# #Feature path
# feature_extraction_dir = artifacts['feature_extraction_dir']
# extracted_features_name = artifacts['extracted_features_name']

# feature_extraction_path = os.path.join(artifacts_dir, feature_extraction_dir)
# features_name = os.path.join(feature_extraction_path, extracted_features_name)

# #params_path
# model_name = params['base']['BASE_MODEL']
# include_tops = params['base']['include_top']
# input_shapes = params['base']['input_shape']
# poolings = params['base']['pooling']


# model = ResNet50(include_top=include_tops,input_shape=(224,224,3),pooling=poolings)
# feature_list = pickle.load(open(features_name,'rb'))
# filenames = pickle.load(open(pickle_file,'rb'))

# def recommend(feature_list,features):
#     similarity = []
#     for i in range(len(feature_list)):
#         similarity.append(cosine_similarity(features.reshape(1, -1), feature_list[i].reshape(1, -1))[0][0])

#     index_pos = sorted(list(enumerate(similarity)), reverse=True, key=lambda x: x[1])[0][0]
#     return index_pos


# def extract_features(image_path, model):
#     # Load the image using OpenCV
#     img = cv2.imread(image_path)
    
#     # Convert the image to RGB format (required by face_recognition)
#     img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    
#     # Find all face locations in the image
#     face_locations = face_recognition.face_locations(img_rgb)

#     if len(face_locations) == 0:
#         return None  # No face found in the image

#     # Use the first detected face
#     top, right, bottom, left = face_locations[0]
#     face_image = img[top:bottom, left:right]

#     # Resize the face image to the required size
#     resized_face_image = cv2.resize(face_image, (224, 224))

#     # Preprocess the resized image for the model
#     preprocessed_img = preprocess_input(np.expand_dims(resized_face_image, axis=0))

#     # Extract features using the model
#     features = model.predict(preprocessed_img).flatten()
#     return features

# # def extract_features(image, model):
# #     # Find all face locations in the image
# #     face_locations = face_recognition.face_locations(image)

# #     if len(face_locations) == 0:
# #         return None  # No face found in the image

# #     # Use the first detected face
# #     top, right, bottom, left = face_locations[0]
# #     face_image = image[top:bottom, left:right]

# #     # Resize the face image to the required size
# #     resized_face_image = cv2.resize(face_image, (224, 224))

# #     # Preprocess the resized image for the model
# #     preprocessed_img = preprocess_input(np.expand_dims(resized_face_image, axis=0))

# #     # Extract features using the model
# #     features = model.predict(preprocessed_img).flatten()
# #     return features


# # def save_uploaded_image(uploaded_image):
# #     try:
# #         create_directory(dirs=[uploadn_path])
# #         image_path = os.path.join(uploadn_path, uploaded_image.name)
# #         with open(image_path, 'wb') as f:
# #             f.write(uploaded_image.getbuffer())
# #         return image_path
# #     except Exception as e:
# #         print("Error saving uploaded image:", e)
# #         return ""

# def save_uploaded_image(uploaded_image):
#     try:
#         create_directory(dirs=[uploadn_path])

#         image_path = os.path.join(uploadn_path, uploaded_image.filename)
#         uploaded_image.save(image_path)

#         return image_path
#     except Exception as e:
#         print("Error saving uploaded image:", e)
#         return ""


#flask_app.py



# from flask import Flask, render_template,request,redirect,url_for,Response
# import cv2 
# from src.utils.all_utils import allowed_file
# from flask_app_functions import save_uploaded_image, recommend,read_yaml,extract_features
# import os 
# # from tensorflow.keras.applications.vgg16 import preprocess_input
# # from tensorflow.keras.applications.vgg16 import VGG16
# from src.utils.all_utils import read_yaml, create_directory
# import pickle
# from sklearn.metrics.pairwise import cosine_similarity
# from PIL import Image
# import numpy as np
# import base64
# from tensorflow.keras.applications import ResNet50
# from tensorflow.keras.applications.resnet50 import preprocess_input


# config = read_yaml('config/config.yaml')
# params = read_yaml('params.yaml')

# artifacts = config['artifacts']
# artifacts_dir = artifacts['artifacts_dir']

# app = Flask(__name__)

# static_dir=artifacts['static_dir']
# #upload
# upload_image_dir = artifacts['upload_image_dir']#the folder where uploaded image will be stored ,uploads
# uploadn_path = os.path.join(static_dir, upload_image_dir)

# # pickle_format_data_dir
# pickle_format_data_dir = artifacts['pickle_format_data_dir']
# img_pickle_file_name = artifacts['img_pickle_file_name']

# raw_local_dir_path = os.path.join(artifacts_dir, pickle_format_data_dir)
# pickle_file = os.path.join(raw_local_dir_path, img_pickle_file_name)

# #Feature path
# feature_extraction_dir = artifacts['feature_extraction_dir']
# extracted_features_name = artifacts['extracted_features_name']

# feature_extraction_path = os.path.join(artifacts_dir, feature_extraction_dir)
# features_name = os.path.join(feature_extraction_path, extracted_features_name)

# #params_path
# model_name = params['base']['BASE_MODEL']
# include_tops = params['base']['include_top']
# input_shapes = params['base']['input_shape']
# poolings = params['base']['pooling']


# model = ResNet50(include_top=include_tops,input_shape=(224,224,3),pooling=poolings)
# feature_list = pickle.load(open(features_name,'rb'))
# filenames = pickle.load(open(pickle_file,'rb'))


# app = Flask(__name__)


# @app.route("/")
# def home():
#     return render_template("index.html")

# @app.route("/choose_computer", methods=["GET", "POST"])
# def choose_computer():
#     if request.method == "POST":
#         uploaded_image = request.files["uploaded_image"]
#         if uploaded_image and allowed_file(uploaded_image.filename):
#             image_path = save_uploaded_image(uploaded_image)
#             if image_path:
#                 features = extract_features(image_path, model)
#                 if features is not None:
#                     index_pos = recommend(feature_list, features)
#                     predicted_actor = " ".join(filenames[index_pos].split("/")[2].split(".")[0])

#                     # Render the template with results
#                     return render_template(
#                         'choose_computer.html',
#                         uploaded=True,
#                         uploaded_image=uploaded_image.filename,
#                         predicted_actor=predicted_actor,
#                         actor_image=filenames[index_pos]
#                     )

#     return render_template("choose_computer.html", uploaded=False)


# def generate_frames(camera):
#     while True:
#         success, frame = camera.read()
#         if not success:
#             break
#         else:
#             ret, buffer = cv2.imencode('.jpg', frame)
#             frame = buffer.tobytes()
#         yield (b'--frame\r\n'
#                b'Content-Type: image/jpeg\r\n\r\n' + frame + b'\r\n')
        
# @app.route('/choose_webcam')
# def index():
#     return render_template('choose_webcam.html')

# @app.route('/video')
# def video():
#     camera = cv2.VideoCapture(0)  # Initialize the camera here
#     return Response(generate_frames(camera), mimetype='multipart/x-mixed-replace; boundary=frame')


# # ... (your imports and app setup)

# # ... (previous code)

# # @app.route('/capture_frame', methods=['GET', 'POST'])
# # def choose_webcam():
# #     if request.method == 'POST':
# #         frame_name = request.form.get('frame_name')
# #         camera = cv2.VideoCapture(0)  # Initialize the camera here
        
# #         _, frame = camera.read()
# #         folder_path = uploadn_path  # Replace with actual absolute path
# #         os.makedirs(folder_path, exist_ok=True)
        
# #         image_path = os.path.join(folder_path, f'{frame_name}.jpg')
# #         cv2.imwrite(image_path, frame)
        
# #         # Perform prediction on the captured frame
# #         features = extract_features(image_path, model)
# #         if features is not None:
# #             index_pos = recommend(feature_list, features)
# #             predicted_actor = " ".join(filenames[index_pos].split("/")[2].split(".")[0])
# #             predicted_actor_image = filenames[index_pos]
# #         else:
# #             predicted_actor = "Unknown"
# #             predicted_actor_image = ""  # Provide a default image path
        
# #         camera.release()  # Release the camera after capturing the frame
        
# #         # Render the results page with captured frame and predicted actor's image
# #         return render_template(
# #             'results.html',
# #             captured_frame=image_path,
# #             predicted_actor=predicted_actor,
# #             predicted_actor_image=predicted_actor_image
# #         )
    
# #     return render_template("choose_webcam.html")

# # ... (other routes and app run







# @app.route('/capture_frame', methods=['POST'])
# def capture_frame():
#     frame_name = request.form['frame_name']
#     camera = cv2.VideoCapture(0)
#     _, frame = camera.read()
#     folder_path = uploadn_path  # Replace with actual absolute path
#     os.makedirs(folder_path, exist_ok=True)
    
#     image_path = os.path.join(folder_path, f'{frame_name}.jpg')
#     cv2.imwrite(image_path, frame)
    
#     # Perform prediction on the captured frame
#     features = extract_features(image_path, model)
#     if features is not None:
#         index_pos = recommend(feature_list, features)
#         predicted_actor = " ".join(filenames[index_pos].split("/")[2].split(".")[0])
#         predicted_actor_image = filenames[index_pos]
#         _, buffer = cv2.imencode('.jpg', frame)
#         captured_frame_base64 = base64.b64encode(buffer).decode('utf-8')#The base64 module provides functions to encode binary data (like image data) into a base64 encoded string, which can then be included in the HTML template.
#     else:
#         predicted_actor = "Unknown"
#         predicted_actor_image = ""  
#         camera.release()
    
#     return render_template(
#         'results.html',
#         captured_frame=captured_frame_base64,
#         predicted_actor=predicted_actor,
#         predicted_actor_image=predicted_actor_image
#     )
    
#     return render_template("choose_webcam.html")

    
# # ... (other routes and app run)


# if __name__ == "__main__":
#     app.run(host="0.0.0.0", debug=True)

#feature_extractions


# from src.utils.all_utils import read_yaml, create_directory
# import argparse
# import os
# import logging
# import tensorflow
# from tensorflow.keras.preprocessing import image
# # from tensorflow.keras.applications.vgg16 import preprocess_input
# # from tensorflow.keras.applications.vgg16 import VGG16
# import numpy as np
# import pickle
# from tqdm import tqdm # for displaying progress bars during iteration.
# from keras_vggface.vggface import VGGFace
# # from tensorflow.python.keras.utils import layer_utils
# from keras_vggface.utils import preprocess_input
# # from tensorflow.keras.applications.resnet50 import ResNet50
# # from tensorflow.keras.applications.resnet50 import preprocess_input

# logging_str = "[%(asctime)s: %(levelname)s: %(module)s]: %(message)s"
# log_dir = "logs"
# os.makedirs(log_dir, exist_ok=True)
# logging.basicConfig(filename=os.path.join(log_dir, 'running_logs.log'), level=logging.INFO, format=logging_str,
#                     filemode="a")
# #the above is the logging syntax



# def extractor(img_path,model):
#     img = image.load_img(img_path,target_size=(224,224))#our original image was 64*64 and this process resizes it to 224*224
#     img_array = image.img_to_array(img)
#     expanded_img = np.expand_dims(img_array,axis=0)#this creates a 4d output adding the batch size to our already 3d array height width and channel , this is done to match the input required by vgg16
#     preprocessed_img = preprocess_input(expanded_img)

#     result = model.predict(preprocessed_img).flatten()#converts the output to 1d array 

#     return result


# def feature_extractor(config_path,params_path):
#     config = read_yaml(config_path)
#     params = read_yaml(params_path)

#     artifacts = config['artifacts']#this is the key of the entire folder

#     artifacts_dir = artifacts['artifacts_dir']#this gives us artifacts as value and this is the name of the folder
#     pickle_format_data_dir = artifacts['pickle_format_data_dir']#this gives us the name pickle_format_data
#     img_pickle_file_name = artifacts['img_pickle_file_name']#this gives us the name img_PICKLE_file.pkl

#     img_pickle_file = os.path.join(artifacts_dir, pickle_format_data_dir, img_pickle_file_name)#this is the path to image_PICKLE_file.pkl


#     filenames = pickle.load(open(img_pickle_file,'rb'))#here we open our img_PICKLE_file.pkl in read binary mode

#     model_name = params['base']['BASE_MODEL']
#     include_tops = params['base']['include_top']
#     input_shapes = params['base']['input_shape']
#     poolings = params['base']['pooling']#all of the above give us the parameters we want the model to have just that we are accessing it through yaml file

#     model = VGGFace(model=model_name ,include_top=include_tops,input_shape=(224,224,3),pooling=poolings)

#     feature_extraction_dir = artifacts['feature_extraction_dir']#this is the name of the directory where our extractered featues will be stored
#     extracted_features_name = artifacts['extracted_features_name']#this will give us the file name embedding.pkl

#     feature_extraction_path = os.path.join(artifacts_dir, feature_extraction_dir)#path to our extracter_features directory
#     create_directory(dirs=[feature_extraction_path])

#     features_name = os.path.join(feature_extraction_path, extracted_features_name)#path to embedding.pkl

#     features = []#this list will contain all the image features array

#     for file in tqdm(filenames):
#         features.append(extractor(file,model))

#     pickle.dump(features,open(features_name,'wb'))




# if __name__ == '__main__':
#     args = argparse.ArgumentParser()
#     args.add_argument("--config", "-c", default="config/config.yaml")
#     args.add_argument("--params", "-p", default="params.yaml")
#     parsed_args = args.parse_args()
    
#     try:
#         logging.info(">>>>> stage_02 started")
#         feature_extractor(config_path = parsed_args.config, params_path= parsed_args.params)
#         logging.info("stage_02 completed!>>>>>")
#     except Exception as e:
#         logging.exception(e)
#         raise e
    





